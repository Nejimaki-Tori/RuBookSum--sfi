@inproceedings{BookSum,
    title = "{BOOKSUM}: A Collection of Datasets for Long-form Narrative Summarization",
    author = "Kryscinski, Wojciech  and
      Rajani, Nazneen  and
      Agarwal, Divyansh  and others",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.488/",
    doi = "10.18653/v1/2022.findings-emnlp.488",
    pages = "6536--6558",
    abstract = "The majority of existing text summarization datasets include short-form source documents that lack long-range causal and temporal dependencies, and often contain strong layout and stylistic biases. While relevant, such datasets will offer limited challenges for future text summarization systems. We address these issues by introducing BOOKSUM, a collection of datasets for long-form narrative summarization. Our dataset covers documents from the literature domain, such as novels, plays and stories, and includes highly abstractive, human written summaries on three levels of granularity of increasing difficulty: paragraph-, chapter-, and book-level. The domain and structure of our dataset poses a unique set of challenges for summarization systems, which include: processing very long documents, non-trivial causal and temporal dependencies, and rich discourse structures. To facilitate future work, we trained and evaluated multiple extractive and abstractive summarization models as baselines for our dataset."
}

@inproceedings{alexandria,
    title = "Echoes from Alexandria: A Large Resource for Multilingual Book Summarization",
    author = "Scir{\`e}, Alessandro  and
      Conia, Simone  and
      Ciciliano, Simone  and
      Navigli, Roberto",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.54/",
    doi = "10.18653/v1/2023.findings-acl.54",
    pages = "853--867",
    abstract = "In recent years, research in text summarization has mainly focused on the news domain, where texts are typically short and have strong layout features. The task of full-book summarization presents additional challenges which are hard to tackle with current resources, due to their limited size and availability in English only. To overcome these limitations, we present ``Echoes from Alexandria'', or in shortened form, ``Echoes'', a large resource for multilingual book summarization. Echoes featuresthree novel datasets: i) Echo-Wiki, for multilingual book summarization, ii) Echo-XSum, for extremely-compressive multilingual book summarization, and iii) Echo-FairySum, for extractive book summarization. To the best of our knowledge, Echoes {--} with its thousands of books and summaries {--} is the largest resource, and the first to be multilingual, featuring 5 languages and 25 language pairs. In addition to Echoes, we also introduce a new extractive-then-abstractive baseline, and, supported by our experimental results and manual analysis of the summaries generated, we argue that this baseline is more suitable for book summarization than purely-abstractive approaches. We release our resource and software at \url{https://github.com/Babelscape/echoes-from-alexandria} in the hope of fostering innovative research in multilingual booksummarization."
}

@inproceedings{fables,
  title={FABLES: Evaluating faithfulness and content selection in book-length summarization},
  author={Kim, Yekyung and Chang, Yapei and Karpinska, Marzena and others},
  booktitle={First Conference on Language Modeling},
  year="2024"
}


@misc{Briefly,
  author       = {{Narodny Briefly}},
  title        = {Digital library of short summaries of literary works},
  year         = {2025},
  howpublished = {\url{https://wiki.briefly.ru/}},
  note         = {Accessed: 2025-07-30}
}

@misc{librusec,
  author       = {{LibRusEc}},
  title        = {Library of works of art},
  year         = {2025},
  howpublished = {\url{https://librusec.org/}},
  note         = {Accessed: 2025-07-30}
}

@misc{hierarchical,
      title={Recursively Summarizing Books with Human Feedback}, 
      author={Jeff Wu and Long Ouyang and Daniel M. Ziegler and others},
      year={2021},
      eprint={2109.10862},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.10862}, 
}

@inproceedings{blueprint,
    title = "Text-Blueprint: An Interactive Platform for Plan-based Conditional Generation",
    author = "Huot, Fantine  and
      Maynez, Joshua  and
      Narayan, Shashi  and
      others",
    editor = "Croce, Danilo  and
      Soldaini, Luca",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-demo.13/",
    doi = "10.18653/v1/2023.eacl-demo.13",
    pages = "105--116",
    abstract = "While conditional generation models can now generate natural language well enough to create fluent text, it is still difficult to control the generation process, leading to irrelevant, repetitive, and hallucinated content. Recent work shows that planning can be a useful intermediate step to render conditional generation less opaque and more grounded. We present a web browser-based demonstration for query-focused summarization that uses a sequence of question-answer pairs, as a blueprint plan for guiding text generation (i.e., what to say and in what order). We illustrate how users may interact with the generated text and associated plan visualizations, e.g., by editing and modifying the plan in order to improve or control the generated output.A short video demonstrating our system is available at \url{https://goo.gle/text-blueprint-demo}"
}

@inproceedings{rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013/",
    pages = "74--81"
}

@inproceedings{bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and others},
  booktitle={International Conference on Learning Representations},
  year="2025"
}

@misc{qwen3,
      title={Qwen3 Technical Report}, 
      author={An Yang and others},
      year={2025},
      eprint={2505.09388},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388}, 
}

@article{ruadapt, title={Facilitating Large Language Model Russian Adaptation with Learned Embedding Propagation}, volume={10}, url={https://jle.hse.ru/article/view/22224}, DOI={10.17323/jle.2024.22224}, abstractNote={&lt;p&gt;&lt;strong&gt;Background: &lt;/strong&gt;Recent advancements in large language model (LLM) technologies have introduced powerful open-source instruction-tuned LLMs that match the text generation quality of leading models like GPT-4. Despite accelerating LLM adoption in sensitive-information environments, the lack of disclosed training data hinders replication and makes these achievements exclusive to specific models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Purpose: &lt;/strong&gt;Given the multilingual nature of the latest iteration of open-source LLMs, the benefits of training language-specific LLMs diminish, leaving computational efficiency as the sole guaranteed advantage of this computationally-expensive procedure. This work aims to address the language-adaptation limitations posed by restricted access to high-quality instruction-tuning data, offering a more cost-effective pipeline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Method: &lt;/strong&gt;To tackle language-adaptation challenges, we introduce Learned Embedding Propagation (LEP), a novel method with lower training data requirements and minimal disruption of existing LLM knowledge. LEP employs an innovative embedding propagation technique, bypassing the need for instruction-tuning and directly integrating new language knowledge into any instruct-tuned LLM variant. Additionally, we developed Darumeru, a new benchmark for evaluating text generation robustness during training, specifically tailored for Russian adaptation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results: &lt;/strong&gt;We applied the LEP method to adapt LLaMa-3-8B and Mistral-7B for Russian, testing four different vocabulary adaptation scenarios. Evaluation demonstrates that LEP achieves competitive performance levels, comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct. Further improvements were observed through self-calibration and additional instruction-tuning steps, enhancing task-solving capabilities beyond the original models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion: &lt;/strong&gt;LEP offers a viable and efficient alternative to traditional language-specific instruction-tuning, significantly reducing the costs associated with language adaptation while maintaining or surpassing the performance benchmarks set by contemporary LLMs.&lt;/p&#38;gt;}, number={4}, journal={Journal of Language and Education}, author={Tikhomirov, Mikhail and Chernyshov, Daniil}, year={2024}, month={Dec.}, pages={130-145} }

@article{deepseek,
  author    = {Aixin Liu and others},
  title     = {DeepSeek{-}V3 Technical Report},
  journal   = {CoRR},
  year      = {2024}
}

@misc{tpro,
  author       = {{T{-}Bank}},
  title        = {T{-}Bank has opened access to its own Russian{-}language language model in the 7--8 billion parameter weight category},
  year         = {2024},
  howpublished = {\url{https://www.tbank.ru/about/news/20072024-t-bank-opened-access-its-own-russian-language-language-model-weight-category-of-7-8-billion-parameters/}},
  note         = {Accessed: 2025-08-21}
}

@misc{yagpt,
  author       = {{Yandex}},
  title        = {YandexGPT 5 with reasoning mode},
  year         = {2025},
  howpublished = {\url{https://ya.ru/ai/gpt}},
  note         = {Accessed: 2025-07-30}
}